 #定义 #方法论 
# 一、模型选择的基本思想

模型选择的核心思想就是从某个模型中选择最佳模型。模型选择主要用于解决**过拟合现象**。

它与一般的“调参”不一样，调参很多时候可能是针对优化算法中的某些参数进行调整，比如步长（学习速率）、迭代次数等，也会涉及到模型中调整参数（也称正则参数）的选择。但是模型选择不涉及算法中的参数，仅涉及模型目标函数中的调整参数。

模型选择最标准的方法是在训练集上训练模型，然后在验证集上获取预测误差，该误差也被称作“样本外（extra-sample）误差”，可真实反映出模型在样本外的预测能力，最后选择最小预测误差所对应的模型作为最佳模型即可。

但通常来说，我们没有独立的验证集，手头仅有的信息就是训练集，那么想要估计测试误差，就只能在训练集上寻找突破，一般来说有以下两种思路：

- **从训练集中划分出一部分数据作为验证集，用这个验证集来测试误差。**
- **对训练误差进行某种转化，来近似测试误差。**

## 思路1：

第一种思路是自然而然的思路。只要对训练集进行合适的划分，就可能近似出预测误差。但是对原始数据的训练集划分为新的训练集和验证集，不同比例的划分可能使得新训练集与原训练集相差较大，进而使得差异很大，用这种方式来估计条件期望形式的预测误差比较困难。

既然如此，我们可以转为估计平均预测误差，通过重复抽样的方式来多次估计预测误差，然后取平均值。这种方式被称为“重复抽样法”。重复抽样法**即通过训练集多次切分、抽样来模拟训练集、验证集，计算多个“样本外误差”，然后求其平均预测误差，这是一种密集计算型方法，比如交叉验证（Cross Validation）、自助法（Bootstrap）等**。

## 思路2：

第二种思路相比于第一种思路更加考虑计算效率。因为重复抽样需要计算多次估计，因此做一次模型选择可能需要花费不少时间。**如果单单从训练集的训练误差就可以近似测试误差，那么模型选择效率便会大大提高。这种方式以统计学中的AIC、BIC等为代表**，深刻剖析了训练误差与样本内（in-sample）误差、预测误差之间的关系，给出了预测误差估计的解析式，因此第二种思路称为“解析法”。

在这里不得不提到**正则化**。模型选择的另一典型方法是正则化（regularization）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项越大。正则化项一般是模型参数向量的范数Ln，也就是L系类正则化。

其实AIC、BIC也是一种正则化，但它们多用于结构选择，而在机器学习领域，L系类正则化主要用于参数学习。L1范数、L2范数是在模型训练的过程中通过正则项来控制参数（或者说特征）的个数，达到防止模型过拟合的问题；AIC、BIC是在已经训练出来的模型中选择更好的那个模型时的判断准则。它们的共性都是为了找到更好的模型。区别就是L1范数、L2范数在训练模型的过程中通过增加约束来找到更好的模型；而AIC和BIC是在已经训练好的不同模型里面筛选出相对最好的那个模型，目的不同，最终的结果也一定有所差距。

以上两种思路在统计学习和机器学习中都有大量应用，相比较而言，统计学习更喜欢第二种解析法，这样容易计算，并且会有较好的理论性质（似然角度）；而机器学习则更喜欢第一种重复抽样法和结构风险最小化法，不需要计算基于分布的似然，普适性更好。

一般而言模型选择的准则有如下几种：

- 重复抽样与预测稳定性角度：CV(交叉验证)、GCV(广义交叉验证)、Bootstrap(自助法)  
- 似然与模型复杂度角度：AIC(赤池信息准则)、AICc、BIC(贝叶斯信息准则)

# 二、赤池信息准则（AIC）

AIC是评估统计模型复杂度和衡量统计模型拟合优良性（Goodness of fit）的一种标准。AIC建立在信息熵的概念基础上。

AIC越小，模型越好，通常选择AIC最小的模型。

在一般情况下，AIC可以表示为：  

$$AIC=2k-2ln(L) $$

其中：k是参数的数量，L是似然函数。

**k 小意味着模型简洁，L大意味着模型精确，因此在评价模型时兼顾了简洁性和精确性**。

假设条件是模型的误差服从独立正态分布，让n为观测值数目，RSS为残差平方和，那么AIC变为：  

$$AIC=2k+nln(RSS/n)$$

增加自由参数的数目提高了拟合的优良性，AIC鼓励数据拟合的优良性，但尽量避免出现过度拟合的情况。

**模型选择中，优先考虑的模型应是AIC值最小的那一个。假设在n个模型中作出选择，可一次算出n个模型的AIC值，并找出最小AIC值对应的模型作为选择对象。**

AIC的方法就是寻找可以最好地解释数据但包含最少自由参数的模型。

## **AIC准则的评价：**

- AIC准则的第一部分是极大似然函数的对数，是从样本信息对总体信息的反映程度，即模型拟合情况考虑的；第二部分是对模型复杂度的惩罚，达到满足模型有效性和可靠性条件下参数个数最少。 它既考虑了模型的拟合情况，又考虑了复杂度情况，采用在同等拟合优度条件下参数最少的模型作为估计模型。
- AIC准则突破了以往仅从模型拟合情况的评价标准，其出发点是最小化k-L距离（相对熵），需要同时满足有效性、可靠性和经济性。AIC值越小，估计概率分布越接近真实分布。

# 三、贝叶斯信息准则（BIC）

贝叶斯决策理论是在不完全情况下，对部分未知的状态用主观概率估计，然后用贝叶斯公式对发生概率进行修正，最后再利用期望值和修正概率做出最优决策。

贝叶斯信息准则的表达式为：  

$$BIC=ln(n)k-2ln(L)$$
其中，k为模型参数个数，n为样本数量，L为似然函数。

ln(n)k惩罚项在维数过大且训练样本数据相对较少的情况下，可以有效避免出现维度灾难现象。

与AIC相似，训练模型时，增加参数数量，也就是增加模型复杂度，会增大似然函数，但是也会导致过拟合现象，针对该问题，AIC和BIC均引入了与模型参数个数相关的惩罚项，BIC的惩罚项比AIC的大，考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高。

**AIC和BIC的原理是不同的，AIC是从预测角度，选择一个好的模型来预测，BIC是从拟合角度，选择一个对现有数据拟合最好的模型**。

## 二者的相同点

构造这些统计量所遵循的统计思想是一致的，就是在考虑拟合残差的同时，依自变量个数施加“惩罚”。

## 不同点

- BIC的惩罚项比AIC大，考虑了样本个数，样本数量多，可以防止模型精度过高造成的模型复杂度过高。
- AIC和BIC前半部分是一样的，BIC考虑了样本数量，样本数量过多时，可以有效防止模型精度过高造成的模型复杂度过高。
- AIC和BIC前半部分是惩罚项，由于BIC比AIC在大数据量时对模型参数惩罚的更多，导致BIC更倾向于选择参数少的简单模型。

# 四、交叉验证

另一种常用的模型选择方法是交叉验证(cross validation)。

如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。 由于验证集有足够多的数据，用它对模型进行选择也是有效的。

但是， 在许多实际应用中数据是不充足的。 为了选择好的模型，可以采用交叉验证方法。

**交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、 测试以及模型选择。**

## 1. 简单交叉验证

简单交叉验证方法是： 首先随机地将已给数据分为两部分， 一部分作为训练集， 另一部分作为测试集（例如， 70%的数据为训练集，30%的数据为测试集）。然后用训练集在各种条件下（例如，不同的参数个数） 训练模型， 从而得到不同的模型； 在测试集上评价各个模型的测试误差， 选出测试误差最小的模型。

## 2. S折交叉验证

应用最多的是S折交叉验证（S-fold cross validation） ， 方法如下：

- 首先随机地将已给数据切分为S个互不相交的大小相同的子集；
- 然后利用S-1个子集的数据训练模型， 利用余下的子集测试模型；
- 将这一过程对可能的S种选择重复进行； 最后选出S次评测中平均测试误差最小的模型。

## 3. 留一交叉验证

S折交叉验证的特殊情形是S＝N，称为留一交叉验证（leave-oneout cross validation），往往在数据缺乏的情况下使用。 这里，N是给定数据集的容量。