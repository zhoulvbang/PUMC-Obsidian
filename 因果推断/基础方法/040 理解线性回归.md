#定义 #模型 
# 总体模型

对于多元回归模型

$$y_i|x_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_p x_{pi} + \varepsilon_i|x_i$$

假设样本有n个观测值：

$$Y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}_{n \times 1}$$

$$X = \begin{bmatrix} 1 & x_{11} & x_{12} & \dots & x_{1p} \\ 1 & x_{21} & x_{22} & \dots & x_{2p} \\ \vdots & & & & \\ 1 & x_{n1} & x_{n2} & \dots & x_{np} \end{bmatrix}_{n \times (p+1)}$$

$$\beta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix}_{(p+1) \times 1}$$

$$\varepsilon = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{bmatrix}_{n \times 1}$$

则回归模型可以表示为：

$$Y = X \beta + \varepsilon$$

这个其实并不难理解，最难的部分可能是矩阵乘法 $X \beta$  ，我们可以这样理解一下：

> $X_{n(P+1)}$ 和 $\beta_{(p+1)\times 1}$ 相乘得到一个n行1列的矩阵 $Y$  ，$Y$  第k行的元素 $y_k$ 是 $X$ 第 k 行和  $\beta$ 相乘加上 $\varepsilon$  第 k 行元素 $\varepsilon_k$  的结果：

$$y_k = \begin{bmatrix} 1 & x_{k1} & x_{k2} & \dots & x_{kp} \end{bmatrix} * \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix} + \varepsilon_k$$
这里的 $Y$ 表示因变量向量，$\beta$ 表示总体参数向量，$X$ 表示所有自变量和一列常数 1 组成的矩阵，$\varepsilon$ 表示随机误差。

回归方程将观测到的因变量分解为两个组成部分：
- 一是能够被自变量解释的结构部分；
- 二是不能被自变量解释的随机部分。

但这里所说的结构部分并不一定具有因果性，因此，使用线性回归也不变可以推断因果。

---
# 回归的三种用途

## ”描述“ 数据

回归的第一种常见用途就是“描述”数据，这代表了传统统计学对回归分析的主流观点。

对统计学家来说，回归分析的主要任务有两个：
- 一是求解回归方程，即寻找一条最优回归以尽可能地概括数据特征；
- 二是数据简化（data reduction），即使用最精炼的模型来描述数据。

针对第一个任务，统计学家提出了多种求解回归系数的方法，其中，最小二乘法（OLS）因为具有良好的统计性质备受青睐。
针对第二个任务，统计学家提出了多种评价模型拟合程度的指标和模型筛选方法，如逐步回归法（stepwise regression）在大量潜在的解释变量中寻找对模型最有解释力的变量组合。

但需要注意的是，通过逐步回归得到的模型只是在统计上最优或“最简”的模型，它本身并不一定就是真实的模型。

## “预测”

线性回归的第二种常见用途就是做“预测”，这代表了工程学中非常流行的一种观点，即回归分析应当帮助人们通过已知来预测未知。

与自然科学家和工程学家相比，社会科学家很少做预测，但近年来随着“机器学习”方法的快速发展，预测在社会科学中的重要性不断提升。

对那些以预测为主要目标的回归分析来说，明确的因果关系并不是必须的，事实上，只要能够帮助提高预测的准确性，即便是哪些明知没有因果关系的变量也可以纳入回归方程。

而且在预测研究中，研究者也不用过于关心变量之间因果关系得以发生的具体机制，在很多情况下，得到一个有预测力的“黑箱”（black box）模型就足够了。

## “解释”

线性回归的第三种常见用途就是做“解释”，即通过线性回归来获取自变量对因变量的影响。

随着因果革命的爆发，这种观点对其他领域的影响正在扩大。

需要注意的是，以因果解释为主要目标的回归分析与另外两种回归方法存在很大的差别，主要差别在于在做因果分析的时候，研究者通常不会非常在意模型的 $R^2$  [^1]，因为 $R^2$ 的大小与回归系数是否具有因果解释力没有必然联系。

一个使用观察数据的多元线性回归模型可能包含很多自变量，因此有比较大的 $R^2$ ，但模型的  $R^2$ 大并不代表该模型可以被用来进行因果解释。

---

综上所述，对因果分析来说， $R^2$ 是大还是小并不重要，但是对于以描述和预测为主要目标的因果分析来说， $R^2$ 却是至关重要。

首先，对于描述性研究来说， $R^2$ 越大意味着散点与回归线的距离越近，因此，大的 $R^2$ 总能在更大程度上概括散点的特征。

其次，对于预测研究来说， $R^2$ 也是至关重要，因为  $R^2$ 在很大程度上代表了模型对因变量的预测力。

不过在预测研究中，为了防止“过拟合”（over fit），研究者通常会将数据分为两个部分：
- 一部分用来拟合模型，称作训练集（training set）；
- 另一部分用来评估模型对新数据的预测力，称作“预测集”（test set）。
预测研究更加关心预测集中的 $R^2$，而不是训练集中的 $R^2$。

[^1]: $R^2$ 是回归系数的判定系数（coefficient of determination），它的值介于 0 和 1 之间。$R^2$ 越接近于 1 ，表示因变量的总变异中能被自变量解释的百分比越高，因此，$R^2$ 通常被用来判定模型的拟合程度。

---
# 理解回归假定

为了正确理解和应用回归分析，我们需要区分不同类型的假定。不同的研究目的对回归模型的假设要求也不尽相同，主要可以划分为三类：

## 1. 方程求解假定（Solution Assumptions）

这类假定是为了确保我们可以通过数学方法求出回归系数 $\hat{\beta}$，也就是回归方程中的参数估计值。

最核心的假定是：

- **X矩阵满秩（Full Rank）**：即 $X'X$ 是可逆的，或者说所有解释变量之间不存在完全的线性相关关系（无完全多重共线性）。  
	若 $X$ 不满秩，则无法唯一地求解 $\hat{\beta} = (X'X)^{-1}X'Y$。

因此，在建模前对解释变量进行多重共线性诊断（如方差膨胀因子 VIF）是必要的。

此外，还有一个常见的技术性假定：

- **样本容量大于解释变量数目（$n > p$）**：否则模型将面临欠拟合、参数不可估的问题。

根据一个样本可以建立一个线性方程，所以根据n个样本最多可以建立n个线性方程，而n个线性方程最多可以求解n个未知参数。

由于回归方程包含一个截距项 $\beta_0$ ，所以模型自变量的数量最多有 n-1 个，否则无法获得所有参数的唯一解。

这些假设主要是数值可解性的前提，并不涉及变量之间的统计关系或因果关系。

---

## 2. 因果推断假定（Causal Inference Assumptions）

若回归的目的是“解释”或者“识别因果关系”，就必须满足更强的假设，这些假设源自于因果推断理论（如鲁宾因果模型和Pearl的因果图理论）：

- **可交换性/无混淆性（Unconfoundedness）**：给定控制变量 $X$，因变量 $Y$ 与处理变量 $T$（即我们关心的某个解释变量）之间不再存在潜在混杂因素。
- **稳定性单位处理值假定（SUTVA）**：即个体的潜在结果不受他人处理状态的影响，并且处理是清晰定义的。
- **共变支持（Overlap）**：每种处理状态下个体都具有非零的概率（例如：每个个体都有接受和不接受某处理的可能性）。

此外，在使用OLS进行因果推断时，还通常依赖如下隐含假定：

- **自变量是外生的（Exogeneity）**：即 $E[\varepsilon|X] = 0$，所有的自变量都与误差项相互独立，误差项与解释变量无关。这是OLS估计具有因果解释力的基础条件。

若上述因果假定不成立，回归系数 $\beta_j$ 不能被解释为“在控制其他变量后，$x_j$ 对 $y$ 的因果影响”。

---

## 3. 统计推断假定（Statistical Inference Assumptions）

如果我们希望对回归结果进行统计检验（如检验系数显著性、构建置信区间等），就必须依赖一系列经典线性模型的假定，这些假定统称为“高斯-马尔科夫假定”（Gauss-Markov assumptions）：

1. **线性关系**：因变量与解释变量之间存在线性关系；
2. **同方差性（Homoscedasticity）**：误差项的方差为常数，即 $Var(\varepsilon_i) = \sigma^2$；
3. **误差项独立（No autocorrelation）**：误差项之间相互独立；
4. **误差项期望为零**：$E(\varepsilon_i) = 0$；
5. **解释变量与误差项不相关**：$Cov(X, \varepsilon_i) = 0$；
6. **解释变量固定（或独立于误差）**：在经典回归中，解释变量视为非随机变量，或与误差项独立；
7. **误差项正态分布（用于小样本推断）**：$\varepsilon \sim N(0, \sigma^2 I)$，这一假设是t检验、F检验等显著性检验的必要前提，尤其在样本量较小时更加重要。

若上述假定成立，则OLS估计具有“最佳线性无偏估计”（BLUE, Best Linear Unbiased Estimator）性质。

只要样本量足够大，无论误差项是否相同且相互独立，$\hat \beta$ 的抽样分布都渐进服从正态分布，只是其方差-协方差矩阵以及标准误不能通过公式：

$$Var(b)=\sigma^2(X'X)^{-1}$$

进行计算。

对于这个问题，统计学家已经给出了多种校正方式：

- 针对异方差，研究者可以使用怀特提出的异方差稳健标准误 [^1] 计算公式获得正确的标准误，在stata中可以在 `regress` 命令后使用选项 `vce(robust)` 得到；
- 对于自相关，研究者也可以使用聚类稳健标准误来缓解自相关的影响，在stata中，聚类稳健标准误可以通过在 `regress` 命令之后使用选项 `vce(cluster)` 得到；
- 如果研究者可以较为准确地设定异方差或自相关的模式，也可以通过加权最小二乘法（weighted least squares，WLS）获得更有效率的系数估计值并进行正确的统计推断。
- 总而言之，在大样本情况下，统计推断并不是一个难以解决的问题。

当这些假设不满足时，可以采取上述的稳健方法（如白标准误、异方差稳健回归、广义最小二乘法等）或者转向其他统计模型（如广义线性模型、混合效应模型等）。

但是如果研究样本较小，就无法使用渐进理论获得 $\hat \beta$ 的抽样分布，在这种情况下，必须借助 误差项 $\varepsilon_i$ 服从正态分布才能进行统计推断。

在小样本的情况下，研究者可以通过绘制模型残差的分布图来检测其分布是否服从正态分布。

如果不是正态分布，则可以对因变量进行适当的非线性变换，如对数变换。

---

# 三类假定与三种用途的对应关系

|回归用途|是否关注模型求解性|是否关注统计推断|是否关注因果解释|
|---|---|---|---|
|描述|✅ 是|✅ 是|❌ 否|
|预测|✅ 是|⚠️ 有时关注|❌ 否|
|解释|✅ 是|✅ 是|✅ 是|
[^1]: 见 [[异方差稳健标准误：一些实际考虑（一）]]
