
# 线性回归与条件期望函数

在线性回归分析中，我们研究的核心目标是理解因变量 YY 如何随自变量 XX 变化。条件期望函数（Conditional Expectation Function, CEF）定义为：

$$E(Yi∣Xi)=f(Xi)E(Y_i|X_i) = f(X_i)$$

它是给定 $X_i$ 条件下 $Y_i$ 的期望，代表真实的数据生成机制（DGP），可以通过穷尽 $X_i$ 的所有取值并计算 $X_i$ 取不同值时 $Y_i$ 的总体均值，这就是 $E(Yi∣Xi)$ 。

在理想情况下，若能准确建模 CEF，就能实现最优预测和推断。

然而，在实际应用中我们往往采用如下线性形式的近似：

$$E(Y_i|X_i) \approx X_i^\top \beta$$

此时，线性回归的作用是以最小均方误差（MSE）的准则，在线性函数类中对 $f(X_i)$ 进行最优逼近。若条件期望函数本身就是线性的，那么 $\beta$ 即为真正的结构参数；否则，$\beta$ 则为最优线性逼近参数。

## 条件期望函数的三个性质

### 1. 可分性（CEF decomposition property）

条件期望函数可以将随机变量 $Y_i$ 分解为两个独立的组成部分，即：
- 可以由 $X_i$ 解释的部分 $E(Y_i|X_i)$
- 不能由 $X_i$ 解释的分布 $\epsilon_i$ 。

### 2. 预测性（CEF prediction property）

即用 $E(Y_i|X_i)$ 预测 $Y_i$ 可以获得最小预测误差，即 $E(Y_i|X_i)$ 是最佳预测函数。

### 3. 可以把 $Y_i$ 的方差分解为独立的两部分之和：
- 条件期望函数自身的方差
- 随机误差项 $\epsilon_i$ 的方差
$$Var(Y_i)=Var(E(Y_i|X_i))+Var(\epsilon_i)$$

在实际研究中，研究者通常会用线性回归估计条件期望函数（Conditional Expectation Function, CEF），这主要有如下两个原因：
### **线性回归提供了对 CEF 的最优线性逼近**

尽管真实的 CEF 可能是非线性的，但在许多情况下，研究者并不要求模型完全正确地刻画 DGP（数据生成机制），而是希望获得一个**在均方误差意义下对 CEF 最优的线性近似**。根据最小二乘原理，OLS 回归估计量满足：

$$\hat{\beta} = \arg\min_{\beta} E[(E(Y|X) - X^\top \beta)^2]$$

即：在线性函数类中，OLS 给出了对 CEF 的最佳逼近。**这意味着即使模型被误设，只要我们关心的是变量之间的一般性线性关系或线性趋势，线性回归仍然是有用的工具**。

此外，线性模型便于解释、计算高效，容易推广到多个变量，有助于进行初步探索性分析（exploratory analysis）。

### **线性回归具有良好的统计性质和渐近行为**

在较弱假设条件下，OLS 估计具有以下优良性质：

- **一致性（Consistency）**：即使在模型误设下，OLS 仍能估计出 CEF 的最优线性逼近参数；
- **渐近正态性（Asymptotic Normality）**：便于进行推断和构建置信区间；
- **无偏性（在零条件期望假设下）**：若 E(ϵ∣X)=0E(\epsilon|X)=0，则 OLS 是无偏的；
- **BLUE（最小方差线性无偏估计）**：在 Gauss-Markov 条件下，OLS 是最有效的线性无偏估计量。

这使得 OLS 在有限样本和大样本下均具有稳健的统计性能，成为最常见的估计方法之一。

---

# 模型误设偏差（Model Misspecification Bias）

模型误设偏差是指模型中由于结构假设不成立而导致自变量和因变量之间的函数关系产生了偏差，主要危害是无法通过线性回归得到条件期望函数的正确表达式，进而错误地估计 $X_i$ 对 $Y_i$ 的影响。

主要来源包括：

1. **函数形式误设（Functional Form Misspecification）**  
如果真实的 CEF 是非线性的，但我们错误地使用了线性模型，则即使估计量无偏，其结果也无法反映真实关系。此时 OLS 所估计的是对真实 CEF 的“线性投影”，而非真实因果效应。例如：
$$\text{True: } E(Y|X) = \sin(X),\quad \text{Model: } E(Y|X) = \beta_0 + \beta_1 X$$
这种情况下模型的残差将带有系统性结构，违反了经典回归中误差项与解释变量独立的假设。

2. **遗漏变量偏差（Omitted Variable Bias）**  
当重要解释变量未被纳入模型，且这些遗漏变量与已包含变量相关时，估计的回归系数将偏离真实值。例如：
$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i,\quad \text{但若 } X_2 \text{ 被遗漏}$$
则 OLS 对 $\beta_1$ 的估计将因 $X_1$ 与 $X_2$ 的相关性而偏误。

3. **测量误差（Measurement Error）**  
若解释变量或因变量测量不准，同样会导致估计偏差，尤其当误差是系统性时（非经典测量误差）。

## 消除模型误设偏差

模型误设偏差（misspecification bias）是指由于回归模型未能正确刻画真实的数据生成机制（DGP），特别是条件期望函数（CEF）非线性却被错误地建模为线性，所带来的系统性估计偏误。

消除模型误设偏差最佳方法是尽可能地将非线性的条件期望函数变换为线性的。

在这一方面，一个有用的分析策略是进行饱和回归（saturated regression）。
### **通过变换或近似将非线性 CEF 线性化**

解决这一问题的根本策略是**使模型更贴近真实的条件期望函数**。具体而言：

- 对变量进行非线性变换（如对数、平方、交互项）；
- 引入高阶多项式（polynomial terms）；
- 使用分段函数或样条函数（splines）；
- 分类变量可转为哑变量，以避免函数形式误设。

### **饱和回归（Saturated Regression）**

饱和回归是一种重要的策略，特别适用于**有限水平的离散型解释变量**。它的基本思想是：

> **为每种解释变量组合设置一个单独的虚拟变量，从而完美拟合样本数据，使条件期望函数在各组合下无模型误设。**

例如，若 $X_1$ 与 $X_2$ 分别为两个取有限个值的分类变量（如教育水平与性别），则饱和模型可表示为：

$$E(Y|X_1, X_2) = \sum_{j=1}^{J} \sum_{k=1}^{K} \alpha_{jk} \cdot \mathbb{1}(X_1 = j, X_2 = k)$$

该模型不对因变量的变化趋势作任何形式假设，而是对每种 $(X_1, X_2)$ 组合设定独立的平均水平（即一个哑变量）。因此：

- 在样本中，**饱和模型对 $E(Y|X)$ 的估计是无偏的**；
- 不存在函数形式误设，残差仅来源于个体噪声，而非系统误差；
- 可作为检验其他模型（如线性模型）是否对 CEF 有效近似的**参照标准（benchmark）**。

然而，饱和回归也有局限性：

- 自变量组合太多时，模型会过度复杂，导致过拟合；
- 无法用于连续变量，或变量取值种类很多的情况。

在实践中，研究者常在样本量允许的情况下**将饱和模型作为非参数参照模型**，以检验线性模型是否合理，或构造稳健标准误、选择变量函数形式。

在自变量数量不多且取值类别较少的情况下，饱和回归是可行的，但如果自变量很多或者包含连续取值的自变量，饱和回归就会因为待估参数过多而变得无法估计。

为了避免这种情况，研究者时常需要对自变量与因变量之间的函数关系进行某种限制，如设置线型、二次曲线或三次曲线关系，抑或是对自变量和因变量进行对数变换等。

做出这些限制可以大大减少待估参数的数量，但也会因此引入模型误设偏差，所以在实际研究时，研究者通常会面临一个两难的困境。

针对这个问题，