#模型 #假定 #方法论 

[线性回归与条件期望函数](#线性回归与条件期望函数)
[模型误设偏差（Model Misspecification Bias）](#模型误设偏差(Model-Misspecification-Bias))
[偏差-方差权衡](#偏差-方差权衡)
# 线性回归与条件期望函数

在线性回归分析中，我们研究的核心目标是理解因变量 YY 如何随自变量 XX 变化。条件期望函数（Conditional Expectation Function, CEF）定义为：

$$E(Yi∣Xi)=f(Xi)E(Y_i|X_i) = f(X_i)$$

它是给定 $X_i$ 条件下 $Y_i$ 的期望，代表真实的数据生成机制（DGP），可以通过穷尽 $X_i$ 的所有取值并计算 $X_i$ 取不同值时 $Y_i$ 的总体均值，这就是 $E(Yi∣Xi)$ 。

在理想情况下，若能准确建模 CEF，就能实现最优预测和推断。

然而，在实际应用中我们往往采用如下线性形式的近似：

$$E(Y_i|X_i) \approx X_i^\top \beta$$

此时，线性回归的作用是以最小均方误差（MSE）的准则，在线性函数类中对 $f(X_i)$ 进行最优逼近。若条件期望函数本身就是线性的，那么 $\beta$ 即为真正的结构参数；否则，$\beta$ 则为最优线性逼近参数。

## 条件期望函数的三个性质

### 1. 可分性（CEF decomposition property）

条件期望函数可以将随机变量 $Y_i$ 分解为两个独立的组成部分，即：
- 可以由 $X_i$ 解释的部分 $E(Y_i|X_i)$
- 不能由 $X_i$ 解释的分布 $\epsilon_i$ 。

### 2. 预测性（CEF prediction property）

即用 $E(Y_i|X_i)$ 预测 $Y_i$ 可以获得最小预测误差，即 $E(Y_i|X_i)$ 是最佳预测函数。

### 3. 可以把 $Y_i$ 的方差分解为独立的两部分之和：
- 条件期望函数自身的方差
- 随机误差项 $\epsilon_i$ 的方差
$$Var(Y_i)=Var(E(Y_i|X_i))+Var(\epsilon_i)$$

在实际研究中，研究者通常会用线性回归估计条件期望函数（Conditional Expectation Function, CEF），这主要有如下两个原因：
### **线性回归提供了对 CEF 的最优线性逼近**

尽管真实的 CEF 可能是非线性的，但在许多情况下，研究者并不要求模型完全正确地刻画 DGP（数据生成机制），而是希望获得一个**在均方误差意义下对 CEF 最优的线性近似**。根据最小二乘原理，OLS 回归估计量满足：

$$\hat{\beta} = \arg\min_{\beta} E[(E(Y|X) - X^\top \beta)^2]$$

即：在线性函数类中，OLS 给出了对 CEF 的最佳逼近。**这意味着即使模型被误设，只要我们关心的是变量之间的一般性线性关系或线性趋势，线性回归仍然是有用的工具**。

此外，线性模型便于解释、计算高效，容易推广到多个变量，有助于进行初步探索性分析（exploratory analysis）。

### **线性回归具有良好的统计性质和渐近行为**

在较弱假设条件下，OLS 估计具有以下优良性质：

- **一致性（Consistency）**：即使在模型误设下，OLS 仍能估计出 CEF 的最优线性逼近参数；
- **渐近正态性（Asymptotic Normality）**：便于进行推断和构建置信区间；
- **无偏性（在零条件期望假设下）**：若 E(ϵ∣X)=0E(\epsilon|X)=0，则 OLS 是无偏的；
- **BLUE（最小方差线性无偏估计）**：在 Gauss-Markov 条件下，OLS 是最有效的线性无偏估计量。

这使得 OLS 在有限样本和大样本下均具有稳健的统计性能，成为最常见的估计方法之一。

---

# 模型误设偏差(Model-Misspecification-Bias)

模型误设偏差是指模型中由于结构假设不成立而导致自变量和因变量之间的函数关系产生了偏差，主要危害是无法通过线性回归得到条件期望函数的正确表达式，进而错误地估计 $X_i$ 对 $Y_i$ 的影响。

主要来源包括：

1. **函数形式误设（Functional Form Misspecification）**  
如果真实的 CEF 是非线性的，但我们错误地使用了线性模型，则即使估计量无偏，其结果也无法反映真实关系。此时 OLS 所估计的是对真实 CEF 的“线性投影”，而非真实因果效应。例如：
$$\text{True: } E(Y|X) = \sin(X),\quad \text{Model: } E(Y|X) = \beta_0 + \beta_1 X$$
这种情况下模型的残差将带有系统性结构，违反了经典回归中误差项与解释变量独立的假设。

2. **遗漏变量偏差（Omitted Variable Bias）**  
当重要解释变量未被纳入模型，且这些遗漏变量与已包含变量相关时，估计的回归系数将偏离真实值。例如：
$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i,\quad \text{但若 } X_2 \text{ 被遗漏}$$
则 OLS 对 $\beta_1$ 的估计将因 $X_1$ 与 $X_2$ 的相关性而偏误。

3. **测量误差（Measurement Error）**  
若解释变量或因变量测量不准，同样会导致估计偏差，尤其当误差是系统性时（非经典测量误差）。

## 消除模型误设偏差

模型误设偏差（misspecification bias）是指由于回归模型未能正确刻画真实的数据生成机制（DGP），特别是条件期望函数（CEF）非线性却被错误地建模为线性，所带来的系统性估计偏误。

消除模型误设偏差最佳方法是尽可能地将非线性的条件期望函数变换为线性的。

在这一方面，一个有用的分析策略是进行饱和回归（saturated regression）。
### **通过变换或近似将非线性 CEF 线性化**

解决这一问题的根本策略是**使模型更贴近真实的条件期望函数**。具体而言：

- 对变量进行非线性变换（如对数、平方、交互项）；
- 引入高阶多项式（polynomial terms）；
- 使用分段函数或样条函数（splines）；
- 分类变量可转为哑变量，以避免函数形式误设。

### **饱和回归（Saturated Regression）**

饱和回归是一种重要的策略，特别适用于**有限水平的离散型解释变量**。它的基本思想是：

> **为每种解释变量组合设置一个单独的虚拟变量，从而完美拟合样本数据，使条件期望函数在各组合下无模型误设。**

例如，若 $X_1$ 与 $X_2$ 分别为两个取有限个值的分类变量（如教育水平与性别），则饱和模型可表示为：

$$E(Y|X_1, X_2) = \sum_{j=1}^{J} \sum_{k=1}^{K} \alpha_{jk} \cdot \mathbb{1}(X_1 = j, X_2 = k)$$

该模型不对因变量的变化趋势作任何形式假设，而是对每种 $(X_1, X_2)$ 组合设定独立的平均水平（即一个哑变量）。因此：

- 在样本中，**饱和模型对 $E(Y|X)$ 的估计是无偏的**；
- 不存在函数形式误设，残差仅来源于个体噪声，而非系统误差；
- 可作为检验其他模型（如线性模型）是否对 CEF 有效近似的**参照标准（benchmark）**。

然而，饱和回归也有局限性：

- 自变量组合太多时，模型会过度复杂，导致过拟合；
- 无法用于连续变量，或变量取值种类很多的情况。

在实践中，研究者常在样本量允许的情况下**将饱和模型作为非参数参照模型**，以检验线性模型是否合理，或构造稳健标准误、选择变量函数形式。

在自变量数量不多且取值类别较少的情况下，饱和回归是可行的，但如果自变量很多或者包含连续取值的自变量，饱和回归就会因为待估参数过多而变得无法估计。

为了避免这种情况，研究者时常需要对自变量与因变量之间的函数关系进行某种限制，如设置线型、二次曲线或三次曲线关系，抑或是对自变量和因变量进行对数变换等。

做出这些限制可以大大减少待估参数的数量，但也会因此引入模型误设偏差，所以在实际研究时，研究者通常会面临一个两难的困境。

针对这个问题，如果自变量不多且取值类别较少，可以使用饱和回归；但如果自变量较多或取值类别很多，则需要一句“简约原则”在保留变量间主要关系的基础上使用尽可能精炼的模型。

具体来说，研究者需要从理论和方法两个方面进行努力

- 一方面，研究者需要基于理论判断自变量与因变量之间的函数形态，如果理论认为是线型就设置为线型，如果理论认为是二次曲线的关系则设置为二次曲线。
- 另一方面，研究者也可以使用统计学家提出的各种模型筛选策略挑选对数据拟合最好的模型，如一句 AIC/BIC 等模型拟合指标[^1] 判断是选用包含更多变量的复杂模型还是更加简约的模型。

---
# 偏差-方差权衡

针对研究者通常面临 **“偏差-方差权衡”（bias-variance trade-off）** 的挑战：

- 一方面，为了逼近真实的条件期望函数 E(Y∣X)E(Y|X)，应尽量采用更灵活的模型（如高阶多项式、非参数模型或饱和模型），以减少模型误设偏差（bias）；
- 另一方面，模型灵活度越高，所需估计的参数越多，样本方差（variance）迅速上升，特别是在有限样本下可能导致严重的过拟合甚至不可估（估计矩阵奇异或不满秩）。
### 平衡误设偏差与复杂度：常见策略

为了解决这一困境，研究者发展出一系列折中方案，旨在**在控制模型复杂度的同时尽可能贴近真实的 CEF**，主要包括以下几种做法：

## 1. **半参数或局部线性建模**

如局部线性回归（local linear regression）、广义加性模型（GAM）等，允许某些变量以非线性方式进入模型，而对其他变量仍保持线性结构。例如：

$$E(Y|X_1, X_2) = \alpha + f_1(X_1) + \beta_2 X_2$$

这种方法既能捕捉主要的非线性特征，又避免了饱和建模所需的高维哑变量扩展。

## 2. **分段回归与样条函数（Spline）**

通过将连续变量的取值区间划分为多个区段，并在每个区段中拟合不同的多项式，研究者可以更灵活地建模非线性关系，常见如自然样条（natural spline）、B样条等：

$$E(Y|X) = \sum_{j=1}^{K} \gamma_j B_j(X)$$

这类方法对变量之间的函数形式施加平滑但灵活的限制，是线性与非线性之间的良好折中。

## 3. **变量变换（log、square-root、Box-Cox等）**

如果理论或经验预期某种特定的非线性结构（如对数或幂次关系），则可对变量进行函数变换来“线性化”条件期望函数，例如：

$$\log(Y) = \beta_0 + \beta_1 \log(X)$$

这种变换在计量经济学和医学研究中非常常见，有助于同时满足线性性和误差项正态性等假设。

## 4. **交叉验证与信息准则选择模型结构**

为防止模型过于复杂或过于简单，研究者可使用交叉验证（CV）、AIC、BIC 等方法来选择变量的函数形式与模型复杂度，以实证数据为依据进行合理化建模。

因此，在实际建模过程中，研究者必须在以下两个目标之间找到平衡：

- **逼近真实条件期望函数（减少偏差）**
- **控制模型复杂度与方差（提高估计稳定性）**

这一过程不仅依赖统计方法，也需结合理论预期、经验观察与数据特征。适当使用非线性建模技术与模型比较工具，将有助于降低模型误设偏差而不至于牺牲估计的稳健性与解释力。

---

[^1]: 与 $R^2$ 类似，AIC/BIC 也是判断模型拟合程度的两个常见指标，他们的数值越小，模型拟合越好。与 $R^2$ 相比，AIC/BIC 的优势在于它们会对复杂模型进行较大力度的惩罚，因此在实践中更加常用。[[AIC、BIC 和交叉验]]